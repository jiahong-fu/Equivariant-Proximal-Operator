# A Model-driven Deep Neural Network for Single Image Rain Removal
# https://arxiv.org/abs/2005.01333
#http://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_A_Model-Driven_Deep_Neural_Network_for_Single_Image_Rain_Removal_CVPR_2020_paper.pdf

from model import common
import torch
import torch.nn as nn
import numpy as np
import torch.nn.init as init
import torch.nn.functional as  F
import os
from torch.autograd import Variable
import torch.distributions.laplace
import scipy.io as io
import numpy as np
from model import FCNN_plus as fcnn

def make_model(args, parent=False):
    return Mainnet(args)

# rain kernel  C initialized by the Matlab code "init_rain_kernel.m"
kernel = io.loadmat('./init_kernel.mat')['C9'] # 3*32*9*9
kernel = torch.FloatTensor(kernel)

# filtering on rainy image for initializing B^(0) and Z^(0), refer to supplementary material(SM)
w_x = (torch.FloatTensor([[1.0,1.0,1.0],[1.0,1.0,1.0],[1.0,1.0,1.0]])/9)
w_x_conv = w_x.unsqueeze(dim=0).unsqueeze(dim=0)

class Mainnet(nn.Module):
    def __init__(self, args):
        super(Mainnet,self).__init__()
        self.S = args.stage                                      #Stage number S includes the initialization process
        self.iter = self.S-1                                     #not include the initialization process
        self.num_M = args.num_M
        self.num_Z = args.num_Z

        # Stepsize
        self.etaM = torch.Tensor([1])                                   # initialization
        self.etaX = torch.Tensor([5])                                   # initialization
        self.eta1 = nn.Parameter(self.etaM, requires_grad=True)         # usd in initialization process
        self.eta2 = nn.Parameter(self.etaX, requires_grad=True)         # usd in initialization process
        self.eta11 = self.make_eta(self.iter, self.etaM)                # usd in iterative process
        self.eta12 = self.make_eta(self.iter, self.etaX)                # usd in iterative process

        # Rain kernel
        self.weight0 = nn.Parameter(data=kernel, requires_grad = True)  # used in initialization process
        self.conv = self.make_weight(self.iter, kernel)                 # rain kernel is inter-stage sharing. The true net parameter number is (#self.conv /self.iter)

        # filter for initializing B and Z
        self.w_z_f0 = w_x_conv.expand(self.num_Z, 3, -1, -1)
        self.w_z_f = nn.Parameter(self.w_z_f0, requires_grad=True)

        # proxNet in initialization process
        # self.xnet = Xnet(self.num_Z+3+3)                                  # +3+3 means concat gradient jiahongfu
        self.xnet = Xnet(self.num_Z+3)                                  # 3 means R,G,B channels for color image
        self.mnet = Mnet(self.num_M)
        # proxNet in iterative process
        # self.x_stage = self.make_xnet(self.S, self.num_Z + 3 + 3)         # +3 +3 + 3 means concat gradient and input jiahongfu
        self.x_stage = self.make_xnet(self.S, self.num_Z + 3)
        self.m_stage = self.make_mnet(self.S, self.num_M)
        # fine-tune at the last
        # self.fxnet = Xnet(self.num_Z + 3 + 3)                              # +3 +3 + 3 means concat gradient and input jiahongfu
        self.fxnet = Xnet(self.num_Z + 3)

        # for sparse rain layer
        self.f = nn.ReLU(inplace=True)
        self.taumm = torch.Tensor([1])
        self.tau = nn.Parameter(self.taumm, requires_grad=True)
    def make_xnet(self, iters, channel):
        layers = []
        for i in range(iters):
            layers.append(Xnet(channel))
        return nn.Sequential(*layers)
    def make_mnet(self, iters, num_M):
        layers = []
        for i in range(iters):
            if i==0:
                layers.append(Mnet(num_M))  # num_M + 3 means concat ES jiahongfu
            else:
                layers.append(Mnet(num_M))    # num_M + num_M + 3 means concat gradient and ES jiahongfu
        return nn.Sequential(*layers)
    def make_eta(self, iters,const):
        const_dimadd = const.unsqueeze(dim=0)
        const_f = const_dimadd.expand(iters,-1)
        eta = nn.Parameter(data=const_f, requires_grad = True)
        return eta
    def make_weight(self, iters,const):
        const_dimadd = const.unsqueeze(dim=0)
        const_f = const_dimadd.expand(iters,-1,-1,-1,-1)
        weight = nn.Parameter(data=const_f, requires_grad = True)
        return weight
            
    def forward(self, input):
        # save mid-updating results
         ListB = []
         ListCM = []

        # initialize B0 and Z0 (M0 =0)
         z0 = F.conv2d(input, self.w_z_f, stride =1, padding = 1)              # dual variable z with the channels self.num_Z
         input_ini = torch.cat((input, z0), dim=1)
         out_dual = self.xnet(input_ini)
         B0 = out_dual[:,:3,:,:]
         Z = out_dual[:,3:,:,:]

         # 1st iteration: Updating B0-->M1
         ES = input - B0
         ECM = self.f(ES-self.tau)                                            #for sparse rain layer
         GM = F.conv_transpose2d(ECM, self.weight0/10, stride=1, padding=4)   # /10 for controlling the updating speed
         M = self.m_stage[0](GM)
        #  M_con = self.m_stage[0](torch.cat((GM, ES), dim=1))    # jiahongfu
        #  M = M_con[:,:32,:,:]
         CM = F.conv2d(M, self.conv[1,:,:,:,:]/10, stride =1, padding = 4)    # self.conv[1,:,:,:,:]ï¼šrain kernel is inter-stage sharing
       
         # 1st iteration: Updating M1-->B1
         EB = input - CM
         EX = B0-EB
         GX = EX
         ############## gradient adjust jiahongfu ###################
         x_dual = B0-self.eta2/10*GX
        #  input_dual = torch.cat((B0, GX, Z), dim=1)
         input_dual = torch.cat((x_dual, Z), dim=1)
        ##############################################################
         out_dual = self.x_stage[0](input_dual)
         B = out_dual[:,:3,:,:]
         Z = out_dual[:,3:,:,:]
        #  Z = out_dual[:,3+3:,:,:]       # jiahongfu
         ListB.append(B)
         ListCM.append(CM)

         for i in range(self.iter):
            #  print("###################Iter:", self.iter)
             # M-net
             ES = input - B
             ECM = CM- ES
             GM = F.conv_transpose2d(ECM,  self.conv[1,:,:,:,:]/10, stride =1, padding = 4)
             ########### gradient adjust jiahongfu ############
             input_new = M - self.eta11[i,:]/10*GM
             M = self.m_stage[i+1](input_new)
            #  input_new = torch.cat((M, GM, ES), dim=1)
            #  M = self.m_stage[i+1](input_new)[:, :self.num_M, :, :]
             ##################################################

             # B-net
             CM = F.conv2d(M, self.conv[1,:,:,:,:]/10, stride =1, padding = 4)
             ListCM.append(CM)
             EB = input - CM
             EX = B - EB
             GX = EX
             ############# gradient adjust jiahongfu #####################
             x_dual = B - self.eta12[i,:]/10*GX
             input_dual = torch.cat((x_dual,Z), dim=1)
            #  input_dual = torch.cat((B, GX, Z), dim=1)
             #################################################################
             out_dual  = self.x_stage[i+1](input_dual)
             B = out_dual[:,:3,:,:]
             Z = out_dual[:,3:,:,:]  
            #  Z = out_dual[:,3+3:,:,:]       # jiahongfu
             ListB.append(B)
         out_dual = self.fxnet(out_dual)                # fine-tune
         B = out_dual[:,:3,:,:]
         ListB.append(B)
         return B0, ListB, ListCM

# # proxNet_M
# class Mnet(nn.Module):
#     def __init__(self, channels):
#         super(Mnet, self).__init__()
#         self.channels = channels
#         self.tau0 = torch.Tensor([0.5])
#         self.taum= self.tau0.unsqueeze(dim=0).unsqueeze(dim=0).unsqueeze(dim=0).expand(-1, self.channels,-1,-1)
#         self.tau = nn.Parameter(self.taum, requires_grad=True)                      # for sparse rain map
#         self.f = nn.ReLU(inplace=True)
#         self.resm1 = nn.Sequential(nn.Conv2d(self.channels, self.channels, kernel_size=3, stride = 1, padding= 1, dilation =1),
#                                   nn.BatchNorm2d(self.channels),
#                                   nn.ReLU(),
#                                   nn.Conv2d(self.channels, self.channels, kernel_size=3, stride = 1, padding= 1, dilation = 1),
#                                   nn.BatchNorm2d(self.channels),
#                                    )
#         self.resm2 = nn.Sequential(nn.Conv2d(self.channels, self.channels,kernel_size=3, stride = 1, padding= 1, dilation = 1),
#                                   nn.BatchNorm2d(self.channels),
#                                   nn.ReLU(),
#                                   nn.Conv2d(self.channels, self.channels, kernel_size=3, stride = 1, padding= 1, dilation = 1),
#                                   nn.BatchNorm2d(self.channels),
#                                   )
#         self.resm3 = nn.Sequential(nn.Conv2d(self.channels, self.channels,kernel_size=3, stride = 1, padding= 1, dilation = 1),
#                                   nn.BatchNorm2d(self.channels),
#                                   nn.ReLU(),
#                                   nn.Conv2d(self.channels, self.channels, kernel_size=3, stride = 1, padding= 1, dilation = 1),
#                                   nn.BatchNorm2d(self.channels),
#                                   )
#         self.resm4 = nn.Sequential(nn.Conv2d(self.channels, self.channels, kernel_size=3, stride = 1, padding= 1, dilation = 1),
#                                   nn.BatchNorm2d(self.channels),
#                                   nn.ReLU(),
#                                   nn.Conv2d(self.channels, self.channels, kernel_size=3, stride = 1, padding= 1, dilation = 1),
#                                   nn.BatchNorm2d(self.channels),
#                                    )
#     def forward(self, input):
#         m1  = F.relu(input + self.resm1(input))
#         m2  = F.relu(m1+ self.resm2(m1))
#         m3  = F.relu(m2+ self.resm3(m2))
#         m4  = F.relu(m3+ self.resm4(m3))
#         m_rev =self.f(m4-self.tau)                                     # for sparse rain map
#         return m_rev

# FCNN proxNet_M
class Mnet(nn.Module):
    def __init__(self, channels):
        super(Mnet, self).__init__()
        self.channels = channels
        # self.kernel_size=5
        self.tau0 = torch.Tensor([0.5])
        self.taum= self.tau0.unsqueeze(dim=0).unsqueeze(dim=0).unsqueeze(dim=0).expand(-1, self.channels,-1,-1)
        self.tau = nn.Parameter(self.taum, requires_grad=True)                      # for sparse rain map
        self.f = nn.ReLU(inplace=True)
        self.resm1 = nn.Sequential(nn.Conv2d(self.channels, self.channels, kernel_size=3, stride = 1, padding= 1, dilation =1),
                                  nn.BatchNorm2d(self.channels),
                                  nn.ReLU(),
                                  nn.Conv2d(self.channels, self.channels, kernel_size=3, stride = 1, padding= 1, dilation = 1),
                                  nn.BatchNorm2d(self.channels),
                                   )
        self.resm2 = nn.Sequential(nn.Conv2d(self.channels, self.channels,kernel_size=3, stride = 1, padding= 1, dilation = 1),
                                  nn.BatchNorm2d(self.channels),
                                  nn.ReLU(),
                                  nn.Conv2d(self.channels, self.channels, kernel_size=3, stride = 1, padding= 1, dilation = 1),
                                  nn.BatchNorm2d(self.channels),
                                  )
        self.resm3 = nn.Sequential(nn.Conv2d(self.channels, self.channels,kernel_size=3, stride = 1, padding= 1, dilation = 1),
                                  nn.BatchNorm2d(self.channels),
                                  nn.ReLU(),
                                  nn.Conv2d(self.channels, self.channels, kernel_size=3, stride = 1, padding= 1, dilation = 1),
                                  nn.BatchNorm2d(self.channels),
                                  )
        self.resm4 = nn.Sequential(nn.Conv2d(self.channels, self.channels, kernel_size=3, stride = 1, padding= 1, dilation = 1),
                                  nn.BatchNorm2d(self.channels),
                                  nn.ReLU(),
                                  nn.Conv2d(self.channels, self.channels, kernel_size=3, stride = 1, padding= 1, dilation = 1),
                                  nn.BatchNorm2d(self.channels),
                                   )
    def forward(self, input):
        m1  = F.relu(input + self.resm1(input))
        m2  = F.relu(m1+ self.resm2(m1))
        m3  = F.relu(m2+ self.resm3(m2))
        m4  = F.relu(m3+ self.resm4(m3))
        m_rev =self.f(m4-self.tau)                                     # for sparse rain map
        return m_rev


# # proxNet_B
# class Xnet(nn.Module):
#     def __init__(self, channels):
#         super(Xnet, self).__init__()
#         self.channels = channels
#         self.resx1 = nn.Sequential(nn.Conv2d(self.channels, self.channels, kernel_size=3, stride = 1, padding= 1,  dilation = 1),
#                                   nn.BatchNorm2d(self.channels),
#                                   nn.ReLU(),
#                                   nn.Conv2d(self.channels, self.channels, kernel_size=3, stride = 1, padding= 1,  dilation = 1),
#                                   nn.BatchNorm2d(self.channels),
#                                   )
#         self.resx2 = nn.Sequential(nn.Conv2d(self.channels, self.channels, kernel_size=3, stride = 1, padding= 1, dilation = 1),
#                                   nn.BatchNorm2d(self.channels),
#                                   nn.ReLU(),
#                                   nn.Conv2d(self.channels, self.channels, kernel_size=3, stride = 1, padding= 1, dilation = 1),
#                                   nn.BatchNorm2d(self.channels),
#                                   )
#         self.resx3 = nn.Sequential(nn.Conv2d(self.channels, self.channels, kernel_size=3, stride = 1, padding= 1, dilation = 1),
#                                  nn.BatchNorm2d(self.channels),
#                                   nn.ReLU(),
#                                   nn.Conv2d(self.channels, self.channels,  kernel_size=3, stride = 1, padding= 1, dilation = 1),
#                                   nn.BatchNorm2d(self.channels),
#                                   )
#         self.resx4 = nn.Sequential(nn.Conv2d(self.channels, self.channels, kernel_size=3, stride = 1, padding= 1,  dilation = 1),
#                                   nn.BatchNorm2d(self.channels),
#                                   nn.ReLU(),
#                                   nn.Conv2d(self.channels, self.channels, kernel_size=3, stride = 1, padding= 1,  dilation = 1),
#                                   nn.BatchNorm2d(self.channels),
#                                   )
#     def forward(self, input):
#         x1  = F.relu(input + self.resx1(input))
#         x2  = F.relu(x1 + self.resx2(x1))
#         x3  = F.relu(x2 + self.resx3(x2))
#         x4  = F.relu(x3 + self.resx4(x3))
#         return x4

# FCNN proxNet_B
class Xnet(nn.Module):
    def __init__(self, channels):
        super(Xnet, self).__init__()
        self.channels = channels
        kernel_size = 5
        n_feats=5
        tranNum=8
        inP=kernel_size
        Smooth=False
        act=nn.ReLU(True)
        self.headx = nn.Sequential(fcnn.Fconv_PCA(kernel_size, channels, n_feats, tranNum, inP=kernel_size, padding=(kernel_size-1)//2, ifIni=1, Smooth=Smooth, iniScale=1.0))
        m_body = [
            fcnn.ResBlock(
                fcnn.Fconv_PCA, n_feats, kernel_size, tranNum=tranNum, inP=inP, bn=False, act=act, res_scale=0.1, Smooth=Smooth, iniScale=1.0
            ) for _ in range(4)
        ]
        # self.resx1 = nn.Sequential(nn.Conv2d(self.channels, self.channels, kernel_size=3, stride = 1, padding= 1,  dilation = 1),
        #                           nn.BatchNorm2d(self.channels),
        #                           nn.ReLU(),
        #                           nn.Conv2d(self.channels, self.channels, kernel_size=3, stride = 1, padding= 1,  dilation = 1),
        #                           nn.BatchNorm2d(self.channels),
        #                           )
        # self.resx2 = nn.Sequential(nn.Conv2d(self.channels, self.channels, kernel_size=3, stride = 1, padding= 1, dilation = 1),
        #                           nn.BatchNorm2d(self.channels),
        #                           nn.ReLU(),
        #                           nn.Conv2d(self.channels, self.channels, kernel_size=3, stride = 1, padding= 1, dilation = 1),
        #                           nn.BatchNorm2d(self.channels),
        #                           )
        # self.resx3 = nn.Sequential(nn.Conv2d(self.channels, self.channels, kernel_size=3, stride = 1, padding= 1, dilation = 1),
        #                          nn.BatchNorm2d(self.channels),
        #                           nn.ReLU(),
        #                           nn.Conv2d(self.channels, self.channels,  kernel_size=3, stride = 1, padding= 1, dilation = 1),
        #                           nn.BatchNorm2d(self.channels),
        #                           )
        # self.resx4 = nn.Sequential(nn.Conv2d(self.channels, self.channels, kernel_size=3, stride = 1, padding= 1,  dilation = 1),
        #                           nn.BatchNorm2d(self.channels),
        #                           nn.ReLU(),
        #                           nn.Conv2d(self.channels, self.channels, kernel_size=3, stride = 1, padding= 1,  dilation = 1),
        #                           nn.BatchNorm2d(self.channels),
        #                           )
        self.body = nn.Sequential(*m_body)
        self.out = nn.Sequential(fcnn.Fconv_PCA_out(kernel_size, n_feats, self.channels, tranNum, inP=kernel_size, padding=(kernel_size-1)//2, ifIni=0, Smooth=Smooth, iniScale=1.0))
        # self.resx1 = nn.Sequential(nn.Conv2d(self.channels, self.channels, kernel_size=3, stride=1, padding=1, dilation=1))
    def forward(self, input):
        # x1  = F.relu(input + self.resx1(input))
        # x2  = F.relu(x1 + self.resx2(x1))
        # x3  = F.relu(x2 + self.resx3(x2))
        # x4  = F.relu(x3 + self.resx4(x3))
        x_f = self.headx(input)
        res = self.body(x_f)
        # res = x_f + 0.1*res

        x = self.out(res)
        x = F.relu(input + 0.1 * x)
        # self.

        return x
